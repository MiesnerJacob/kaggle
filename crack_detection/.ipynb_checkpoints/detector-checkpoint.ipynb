{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crack Detection Binary Classification\n",
    "\n",
    "This dataset is made up of images of buildings and structures with cracks and without. The goal of the project is to build a model that is able to distinguish between the two and make highly accurate predictions on new unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# -----------------\n",
    "# Import libararies\n",
    "# -----------------\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from shutil import copyfile\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from torchvision import transforms\n",
    "import splitfolders\n",
    "\n",
    "\n",
    "# -----------------\n",
    "# Extract Zip File\n",
    "# -----------------\n",
    "local_zip = '/Users/miesner.jacob/kaggle/crack_detection/crack-identification-ce784a-2020-iitk.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create file-structure to separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for dirname, _, filenames in os.walk('Users/miesner.jacob/kaggle/crack_detection/crack-identification-ce784a-2020-iitk'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train images 2\n",
      "Total Test images 2000\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# Confirming image counts\n",
    "# -----------------------\n",
    "base_path = '/tmp/'\n",
    "train_dir = os.path.join(base_path, 'train/')\n",
    "test_dir = os.path.join(base_path, 'test/')\n",
    "print(f'Total Train images {len(os.listdir(os.path.join(train_dir)))}')\n",
    "print(f'Total Test images {len(os.listdir(os.path.join(test_dir)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1806 files [00:00, 2876.98 files/s]"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/tmp/com.apple.dyld'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9e65e0532522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'images'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msplitfolders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/splitfolders/split.py\u001b[0m in \u001b[0;36mratio\u001b[0;34m(input, output, seed, ratio, group_prefix)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mclass_dir\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_dirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         split_class_dir_ratio(\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mclass_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/splitfolders/split.py\u001b[0m in \u001b[0;36msplit_class_dir_ratio\u001b[0;34m(class_dir, output, ratio, seed, prog_bar, group_prefix)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \"\"\"Splits one very class folder\n\u001b[1;32m    197\u001b[0m     \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# the data was shuffled already\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/splitfolders/split.py\u001b[0m in \u001b[0;36msetup_files\u001b[0;34m(class_dir, seed, group_prefix)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgroup_prefix\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/splitfolders/split.py\u001b[0m in \u001b[0;36mlist_files\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \"\"\"Returns all files in a given directory\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/splitfolders/split.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \"\"\"Returns all files in a given directory\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     return [\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/tmp/com.apple.dyld'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Copying files: 2000 files [00:19, 2876.98 files/s]"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Extract Zip File & create files\n",
    "# -------------------------------\n",
    "os.mkdir('images') \n",
    "output_folder = 'images'\n",
    "splitfolders.ratio(base_path, output=output_folder, ratio=(.9, .1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Defining directory variables\n",
    "# ----------------------------\n",
    "TRAINING_DIR = os.path.join('images', 'train')\n",
    "TESTING_DIR = os.path.join('images', 'val')\n",
    "\n",
    "train_makeup_dir = os.path.join(TRAINING_DIR, 'makeup')\n",
    "train_nomakeup_dir = os.path.join(TRAINING_DIR, 'no_makeup')\n",
    "\n",
    "test_makeup_dir = os.path.join(TESTING_DIR, 'makeup')\n",
    "test_nomakeup_dir = os.path.join(TESTING_DIR, 'no_makeup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Checking image counts in each directory\n",
    "# ---------------------------------------\n",
    "print('total training makeup images :', len(os.listdir(train_makeup_dir) ))\n",
    "print('total training no makeup images :', len(os.listdir(train_nomakeup_dir) ))\n",
    "\n",
    "print('total test makeup images :', len(os.listdir(test_makeup_dir) ))\n",
    "print('total test no makeup images :', len(os.listdir(test_nomakeup_dir) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Inception V3 model for transfer learning!\n",
    "\n",
    "The reason I am using transfer learning in this problem is that there is not a lot of data to train on, therefore using weights trained for another task and fine tuning it using my data is a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Loading weights and applying to inception model\n",
    "# --------------------------------------------------\n",
    "local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
    "                                include_top=False,\n",
    "                                weights=None)\n",
    "\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Make all layers in pre-trained model not trainable\n",
    "# --------------------------------------------------\n",
    "for layer in pre_trained_model.layers:\n",
    "    pre_trained_model.trainable = False\n",
    "    \n",
    "# --------------------------------------------------\n",
    "# Getting last layer outuput to connect to fine-tuned\n",
    "# DNN attached for transfer learning\n",
    "# --------------------------------------------------\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Defining DNN to train\n",
    "# ---------------------\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(last_output)\n",
    "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(pre_trained_model.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify optimizer, create checkpoint model files, Augment training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Define Optimizer\n",
    "# ----------------\n",
    "adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Saving model using Checkpoint method\n",
    "# ------------------------------------\n",
    "filepath=\"best_model_file.h5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Compiling model\n",
    "# ---------------\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Using Data Augmentation to have more\n",
    "# data to train on and for regularization\n",
    "# ---------------------------------------\n",
    "TRAINING_DIR = 'images/train/'\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    width_shift_range=.5,\n",
    "    height_shift_range=0.5,\n",
    "    zoom_range=.5,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.2,1.0],\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.3\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size=64,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    class_mode='binary',\n",
    "                                                    classes=['makeup','no_makeup'],\n",
    "                                                    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size=64,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    class_mode='binary',\n",
    "                                                    classes=['makeup','no_makeup'],\n",
    "                                                    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Creating Test Generator\n",
    "# -----------------------\n",
    "TEST_DIR = 'images/val'\n",
    "test_datagen = ImageDataGenerator(rescale=1/255)\n",
    "test_generator = test_datagen.flow_from_directory(TEST_DIR,\n",
    "                                                  batch_size=64,\n",
    "                                                  target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --------------\n",
    "# Training Model\n",
    "# --------------\n",
    "history = model.fit(train_generator,\n",
    "                    batch_size=64,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "# ---------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "# ---------------------------------------------------------\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))  # Get number of epochs\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "# -----------------------------------------------\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, acc, '-g', label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, '-b', label=\"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# -------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "# -------------------------------------------\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, loss, '-g', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, '-b', label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Load best model iteration\n",
    "# -------------------------\n",
    "model = tf.keras.models.load_model('best_model_file.h5')\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation on the Test data set\n",
    "# -------------------------------\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
