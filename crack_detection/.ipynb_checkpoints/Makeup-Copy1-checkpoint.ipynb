{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Makeup Binary Classification\n",
    "\n",
    "This dataset is made up of images of people wearing makeup and not wearing makeup. The goal of the project is to build a model that is able to distinguish between the two and make highly accurate predictions on new unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Import libararies\n",
    "# ------------------------------------------------\n",
    "import os\n",
    "import zipfile\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from shutil import copyfile\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Extract Zip File\n",
    "# ------------------------------------------------\n",
    "local_zip = 'makeup.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create file-structure to separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Create file structure for tensorflow to label/split data \n",
    "# ---------------------------------------------------------\n",
    "try:\n",
    "    os.mkdir('/tmp/makeup')\n",
    "    os.mkdir('/tmp/makeup/training')\n",
    "    os.mkdir('/tmp/makeup/testing')\n",
    "    os.mkdir('/tmp/makeup/training/yes_makeup')\n",
    "    os.mkdir('/tmp/makeup/training/no_makeup')\n",
    "    os.mkdir('/tmp/makeup/testing/yes_makeup')\n",
    "    os.mkdir('/tmp/makeup/testing/no_makeup')\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Function to split data and write to directories\n",
    "# ------------------------------------------------\n",
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    all_files = []\n",
    "\n",
    "    for file_name in os.listdir(SOURCE):\n",
    "        file_path = SOURCE + file_name\n",
    "\n",
    "        if os.path.getsize(file_path):\n",
    "            all_files.append(file_name)\n",
    "        else:\n",
    "            print('{} is zero length, so ignoring'.format(file_name))\n",
    "\n",
    "    n_files = len(all_files)\n",
    "    split_point = int(n_files * SPLIT_SIZE)\n",
    "\n",
    "    shuffled = random.sample(all_files, n_files)\n",
    "\n",
    "    train_set = shuffled[:split_point]\n",
    "    test_set = shuffled[split_point:]\n",
    "\n",
    "    for file_name in train_set:\n",
    "        copyfile(SOURCE + file_name, TRAINING + file_name)\n",
    "\n",
    "    for file_name in test_set:\n",
    "        copyfile(SOURCE + file_name, TESTING + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Naming directories as variables\n",
    "# ------------------------------------------------\n",
    "YES_SOURCE_DIR = \"/tmp/data/makeup/\"\n",
    "TRAINING_YES_MAKEUP_DIR = \"/tmp/makeup/training/yes_makeup/\"\n",
    "TESTING_YES_MAKEUP_DIR = \"/tmp/makeup/testing/yes_makeup/\"\n",
    "NO_SOURCE_DIR = \"/tmp/data/no_makeup/\"\n",
    "TRAINING_NO_MAKEUP_DIR = \"/tmp/makeup/training/no_makeup/\"\n",
    "TESTING_NO_MAKEUP_DIR = \"/tmp/makeup/testing/no_makeup/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Function to split data and write to directories\n",
    "# ------------------------------------------------\n",
    "split_size = .9\n",
    "split_data(YES_SOURCE_DIR, TRAINING_YES_MAKEUP_DIR, TESTING_YES_MAKEUP_DIR, split_size)\n",
    "split_data(NO_SOURCE_DIR, TRAINING_NO_MAKEUP_DIR, TESTING_NO_MAKEUP_DIR, split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in Inception V3 model for transfer learning!\n",
    "\n",
    "The reason I am using transfer learning in this problem is that there is not a lot of data to train on, therefore using weights trained for another task and fine tuning it using my data is a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Loading weights and applying to inception model\n",
    "# --------------------------------------------------\n",
    "local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "\n",
    "pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
    "                                include_top=False,\n",
    "                                weights=None)\n",
    "\n",
    "pre_trained_model.load_weights(local_weights_file)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Make all layers in pre-trained model not trainable\n",
    "# --------------------------------------------------\n",
    "for layer in pre_trained_model.layers:\n",
    "    pre_trained_model.trainable = False\n",
    "    \n",
    "# --------------------------------------------------\n",
    "# Getting last layer outuput to connect to fine-tuned\n",
    "# DNN attached for transfer learning\n",
    "# --------------------------------------------------\n",
    "last_layer = pre_trained_model.get_layer('mixed7')\n",
    "print('last layer output shape: ', last_layer.output_shape)\n",
    "last_output = last_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Defining DNN to train\n",
    "# ---------------------\n",
    "x = tf.keras.layers.Flatten()(last_output)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(pre_trained_model.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify optimizer, create checkpoint model files, Augment training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Define Optimizer\n",
    "# ----------------\n",
    "adam = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# Saving model using Checkpoint method\n",
    "# ------------------------------------\n",
    "filepath=\"best_model_file.h5\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=True)\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Compiling model\n",
    "# ---------------\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Using Data Augmentation to have more\n",
    "# data to train on and for regularization\n",
    "# ---------------------------------------\n",
    "TRAINING_DIR = '/tmp/makeup/training'\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1/255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=.1,\n",
    "    height_shift_range=.1,\n",
    "    shear_range=.1,\n",
    "    zoom_range=.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size=32,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    class_mode='binary',\n",
    "                                                    subset='training')\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
    "                                                    batch_size=32,\n",
    "                                                    target_size=(150, 150),\n",
    "                                                    class_mode='binary',\n",
    "                                                    subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Creating Test Generator\n",
    "# ------------------------------\n",
    "TEST_DIR = '/tmp/makeup/testing'\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "test_generator = test_datagen.flow_from_directory(TEST_DIR,\n",
    "                                          batch_size=32,\n",
    "                                          target_size=(150, 150),\n",
    "                                          class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Training Model\n",
    "# ------------------------------\n",
    "history = model.fit(train_generator,\n",
    "                    batch_size=32,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "# -----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "# -----------------------------------------------------------\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))  # Get number of epochs\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "# ------------------------------------------------\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, acc, '-g', label=\"Training Accuracy\")\n",
    "plt.plot(epochs, val_acc, '-b', label=\"Validation Accuracy\")\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# ------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "# ------------------------------------------------\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epochs, loss, '-g', label=\"Training Loss\")\n",
    "plt.plot(epochs, val_loss, '-b', label=\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Load best model iteration\n",
    "# -------------------------------\n",
    "model = keras.models.load_model('best_model_file.h5')\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluation on the Test data set\n",
    "# -------------------------------\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_class = model.predict_classes(test_images[0])\n",
    "prediction = img_class[0]\n",
    "classname = img_class[0]\n",
    "print(\"Class: \",classname)\n",
    "img = img.reshape((28,28))\n",
    "plt.imshow(img)\n",
    "plt.title(classname)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
